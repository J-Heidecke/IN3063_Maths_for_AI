{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for AI - Coursework Task 2\n",
    "\n",
    "The second task is about classifying handwritten digits. We will use the MNIST dataset for training and testing. The point of this task is to develop a multi-layer neural network for classification using mostly Numpy:\n",
    "\n",
    "• Implement sigmoid and relu layers (with forward and backward pass)\n",
    "\n",
    "• Implement a softmax output layer\n",
    "\n",
    "• Implement a fully parameterizable neural network (number and types of layers, number of units)\n",
    "\n",
    "• Implement an optimizer(e.g. SGD or Adam)and a stopping criterion of your choosing\n",
    "• Train your Neural Network using backpropagation. Evaluate different neural network architectures and compare your different results. You can also compare with the results presented in http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import limited libraries\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "df = load_digits()\n",
    "\n",
    "# Divorce data and target \n",
    "X = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset - Inspiration taken from Lab 04 \n",
    "ratio = 0.75\n",
    "length = len(X)\n",
    "\n",
    "X_train = X[:int(length*ratio)]\n",
    "X_test = X[int(length*ratio):]\n",
    "y_train = y[:(length - int(length*ratio))]\n",
    "y_test = y[(length - int(length*ratio)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Dataset\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "# Sigmoid function\n",
    "    \n",
    "def Sigmoid_forwards(inputs):\n",
    "    return 1/(1+np.exp(-inputs))\n",
    "\n",
    "    # Derivative of Sigmoid (for backward propagation) \n",
    "def Sigmoid_backwards(derivative, inputs):\n",
    "    sig = sigmoid_forwards(inputs)\n",
    "    return derivative * sig * (1 - sig)\n",
    "\n",
    "# ReLu function\n",
    "\n",
    "def Relu_forward(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "# Derivative of ReLu\n",
    "def Relu_backward(derivative, inputs):\n",
    "    derivative = np.array(derivative, copy = True)\n",
    "    derivative[inputs <= 0] = 0;\n",
    "    return derivative;\n",
    "\n",
    "# Softmax function for output\n",
    "def Softmax_forward(self):\n",
    "    e = exp(self.inputs)\n",
    "    return e / e.sum()\n",
    "\n",
    "# Derivative of softmax\n",
    "def Softmax_backward(derivative, inputs):\n",
    "    soft = softmax(inputs)\n",
    "    return derivative * soft * (1 - soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def get_cost(target, output):\n",
    "    m = target.shape[1]\n",
    "    cost = -1 / m * (np.dot(output, np.log(target).T) + np.dot(1 - output, np.log(1 - target).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-d2366899a98d>, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-8-d2366899a98d>\"\u001B[0;36m, line \u001B[0;32m84\u001B[0m\n\u001B[0;31m    for layer_number_previous, layer in reversed(list(enumerate(nn_architecture))):\u001B[0m\n\u001B[0m                                                                                  ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, nn_architecture):\n",
    "        \n",
    "        self.nn_architecture = nn_architecture\n",
    "        \n",
    "    def initialize(self):\n",
    "    \n",
    "        memory = {}\n",
    "        \n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_number = idx + 1\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "            memory['Weight' + str(layer_number)] = 0.1 * np.random.randn(\n",
    "            layer_output_size, layer_input_size)\n",
    "            memory['bias' + str(layer_number)] = 0.1 * np.random.randn(\n",
    "            layer_output_size, 1)\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, inputs, weights, biases, activation):\n",
    "    \n",
    "        output_forward = np.dot(inputs, weights) + biases\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            activation_function = Sigmoid_forward()\n",
    "        elif(activation == 'relu'):\n",
    "            activation_function = Relu_forward()\n",
    "        elif(activation == 'softmax'):\n",
    "            activation_function = Softmax_forward()\n",
    "        else:\n",
    "            raise Exception('Activation function not defined')\n",
    "            \n",
    "        return activation_function(output_forward), output_forward\n",
    "    \n",
    "    def forward_full(self, X, memory):\n",
    "        temp = {}\n",
    "        inputs_current = X\n",
    "        \n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_number = idx + 1\n",
    "            inputs_previous = inputs_current\n",
    "            \n",
    "            activation_function_current = layer['activation']\n",
    "            weight_current = memory['Weight' + str(layer_number)]\n",
    "            bias_current = memory['bias' + str(layer_number)]\n",
    "            output_forward, result_forward = self.forward(inputs_previous, weight_current, bias_current,\n",
    "                                                         activation_function_current)\n",
    "            \n",
    "            temp['Output' + str(layer_number)] = output_forward\n",
    "            temp['Result' + str(layer_number)] = result_forward\n",
    "            \n",
    "            \n",
    "        return inputs_current, temp\n",
    "    \n",
    "    def backward(self, inputs):\n",
    "        m = inputs.shape\n",
    "    \n",
    "        if(activation == 'sigmoid'):\n",
    "            activation_function = Sigmoid_backward()\n",
    "        elif(activation == 'relu'):\n",
    "            activation_function = Relu_backward()\n",
    "        elif(activation == 'softmax'):\n",
    "            activation_function = Softmax_backward()\n",
    "        else:\n",
    "            raise Exception('Activation function not defined')\n",
    "            \n",
    "        output_backwards = activation_function()\n",
    "        weight_backwards = np.dot().T / m\n",
    "        bias_backwards = np.sum() / m\n",
    "        resul_backwards\n",
    "    \n",
    "    def full_backward(self, target, output, memory, temp, nn_architecture)\n",
    "    \n",
    "        derived_values = {}\n",
    "        \n",
    "        m = output.shape[1]\n",
    "        output = output.reshape(target.shape)\n",
    "        output_error_previous =  np.divide(1 - output, 1 - target) - (np.divide(output, target)\n",
    "        \n",
    "        for layer_number_previous, layer in reversed(list(enumerate(nn_architecture))):\n",
    "            layer_number_current = layer_number_previous + 1\n",
    "            activation_function = layer[\"activation\"]\n",
    "            \n",
    "            output_error_current = output_error_previous\n",
    "            \n",
    "        outpt_previous = memory[\"Output\" + str(layer_number_previous)]\n",
    "        result_current = memory[\"Result\" + str(layer_number_current)]\n",
    "        weight_current = params_values[\"Weight\" + str(layer_number_current)]\n",
    "        bias_current = params_values[\"bias\" + str(layer_number_current)]\n",
    "        \n",
    "        \n",
    "        error =  np.divide(1 - output, 1 - target) - (np.divide(output, target)\n",
    "    def update_weight(self):\n",
    "        pass;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(nn_architecture)\n",
    "nn.initialize()\n",
    "nn.full_forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = init_layers(nn_architecture)\n",
    "print(layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}