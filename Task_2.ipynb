{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for AI - Coursework Task 2\n",
    "\n",
    "The second task is about classifying handwritten digits. We will use the MNIST dataset for training and testing. The point of this task is to develop a multi-layer neural network for classificationusing mostly Numpy:\n",
    "\n",
    "• Implement sigmoid and relu layers (with forward and backward pass)\n",
    "\n",
    "• Implement a softmax output layer\n",
    "\n",
    "• Implement a fully parameterizable neural network (number and types of layers, number of units)\n",
    "\n",
    "• Implement an optimizer(e.g. SGD or Adam)and a stopping criterionof your choosing\n",
    "• Train your Neural Network using backpropagation. Evaluate different neural network architectures andcompare your different results. You can also compare withthe results presented inhttp://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import limited libraries\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "df = load_digits()\n",
    "\n",
    "# Divorce data and target \n",
    "X = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset - Inspiration taken from Lab 04 \n",
    "ratio = 0.75\n",
    "length = len(X)\n",
    "\n",
    "X_train = X[:int(length*ratio)]\n",
    "X_test = X[int(length*ratio):]\n",
    "y_train = y[:(length - int(length*ratio))]\n",
    "y_test = y[(length - int(length*ratio)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Dataset\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions\n",
    "\n",
    "# Sigmoid function\n",
    "    \n",
    "def Sigmoid_forwards(inputs):\n",
    "    return 1/(1+np.exp(-inputs))\n",
    "\n",
    "    # Derivative of Sigmoid (for backward propagation) \n",
    "def Sigmoid_backwards(derivative, inputs):\n",
    "    sig = sigmoid_forwards(inputs)\n",
    "    return derivative * sig * (1 - sig)\n",
    "\n",
    "# ReLu function\n",
    "\n",
    "def Relu_forward(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "# Derivative of ReLu\n",
    "def Relu_backward(derivative, inputs):\n",
    "    derivative = np.array(derivative, copy = True)\n",
    "    derivative[inputs <= 0] = 0;\n",
    "    return derivative;\n",
    "\n",
    "# Softmax function for output\n",
    "def Softmax_forward(self):\n",
    "    e = exp(self.inputs)\n",
    "    return e / e.sum()\n",
    "\n",
    "# Derivative of softmax\n",
    "def Softmax_backward(derivative, inputs):\n",
    "    soft = softmax(inputs)\n",
    "    return derivative * soft * (1 - soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def get_cost(target, output):\n",
    "    m = target.shape[1]\n",
    "    cost = -1 / m * (np.dot(output, np.log(target).T) + np.dot(1 - output, np.log(1 - target).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, n_layers, n_outputs):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_layers = n_layers\n",
    "        self.n_outputs = self.n_outputs\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeroes((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs, activation)\n",
    "    \n",
    "        output_forward = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "        if(activation == 'sigmoid')\n",
    "            activation_function = Sigmoid_forward()\n",
    "        elif(activation == 'relu')\n",
    "            activation_function = Relu_forward()\n",
    "        elif(activation == 'softmax')\n",
    "            activation_function = Softmax_forward()\n",
    "        else:\n",
    "            raise Exception('Activation function not defined')\n",
    "            \n",
    "        return activation_function.forward(self.output_forward), self.output_forward\n",
    "    \n",
    "    def forward_full(self)\n",
    "        memory = {}\n",
    "        \n",
    "        for i in range self.n_layers\n",
    "            layer_number = i + 1\n",
    "            \n",
    "            \n",
    "            result, output = self.forward\n",
    "    \n",
    "    def backward(self, inputs):\n",
    "        m = inputs.shape\n",
    "    \n",
    "        if(activation == 'sigmoid')\n",
    "            activation_function = Sigmoid_backward()\n",
    "        elif(activation == 'relu')\n",
    "            activation_function = Relu_backward()\n",
    "        elif(activation == 'softmax')\n",
    "            activation_function = Softmax_backward()\n",
    "        else:\n",
    "            raise Exception('Activation function not defined')\n",
    "            \n",
    "        b_activation = activation_function()\n",
    "        b_weight = np.dot().T / m\n",
    "        b_bias = np.sum() / m\n",
    "        b_output\n",
    "        \n",
    "    def update_weight(self)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
