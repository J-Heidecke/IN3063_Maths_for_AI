{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for AI - Coursework Task 2\n",
    "\n",
    "The second task is about classifying handwritten digits. We will use the MNIST dataset for training and testing. The point of this task is to develop a multi-layer neural network for classificationusing mostly Numpy:\n",
    "\n",
    "• Implement sigmoid and relu layers (with forward and backward pass)\n",
    "\n",
    "• Implement a softmax output layer\n",
    "\n",
    "• Implement a fully parameterizable neural network (number and types of layers, number of units)\n",
    "\n",
    "• Implement an optimizer(e.g. SGD or Adam)and a stopping criterionof your choosing\n",
    "• Train your Neural Network using backpropagation. Evaluate different neural network architectures andcompare your different results. You can also compare withthe results presented inhttp://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import limited libraries\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "\n",
    "df = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "X = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture \n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"softmax\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "# Derivative of Sigmoid (for backward propagation) \n",
    "def sigmoid_d(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "# ReLu function\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "# Derivative of ReLu\n",
    "def relu_d(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n",
    "\n",
    "# Softmax function for output\n",
    "def softmax(vector):\n",
    "    e = exp(vector)\n",
    "    return e / e.sum()\n",
    "\n",
    "# Derivative of softmax\n",
    "def softmax_d(dA, Z):\n",
    "    soft = softmax(Z)\n",
    "    return dA * soft * (1 - soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "# Cost function\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "# Accuracy \n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "def init_network(nn_architecture, seed = 9):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_vals = {}\n",
    "\n",
    "    for id, layer in enumerate(nn_architecture):\n",
    "        layer_id = id + 1\n",
    "        liz = layer[\"input\"] # Set layer input size (liz)\n",
    "        loz = layer[\"output\"] # Set layer output size (loz)\n",
    "    \n",
    "        params_vals['W' + str(layer_id)] = np.random.randn(loz, liz) * 0.1 # Set random weight\n",
    "        params_vals['b' + str(layer_id)] = np.random.randn(loz, 1) * 0.1 # Set random bias vector\n",
    "            \n",
    "    return params_vals\n",
    "\n",
    "def single_layer_forward_propagagtion(self, A, W, b, activation):\n",
    "    Z = np.dot(W, A) + b # Calculate Z\n",
    "        \n",
    "    # Logic to decide the activation function\n",
    "    if activation == \"relu\":\n",
    "        activation_function = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_function = relu\n",
    "    elif activation == \"softmax\":\n",
    "        activation_function = relu\n",
    "    else:\n",
    "        raise Expection('Activation function not supported')\n",
    "\n",
    "    return activation_func(Z), Z\n",
    "\n",
    "def full_forward_propagation(X, params_vals, nn_architecture):\n",
    "    mem = {}\n",
    "    A_curr = X\n",
    "        \n",
    "    for id, layer in enumerate(nn_architecture):\n",
    "        layer_id = id + 1\n",
    "        A_prev = A_curr\n",
    "\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_vals['W' + str(layer_id)]\n",
    "        b_curr = params_vals ['b' + str(layer_id)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "            \n",
    "        mem['A' + str(id)] = A_prev\n",
    "        mem['Z' + str(layer_id)] = Z_curr\n",
    "\n",
    "    return A_curr, mem\n",
    "\n",
    "def single_layer_backward_propagation(dA, W, b, Z, A, activation):\n",
    "    m = A.shape[1]\n",
    "\n",
    "    if activation == 'relu':\n",
    "        backward_activation_func = relu_d\n",
    "    elif activation == 'sigmoid':\n",
    "        backward_activation_func = sigmoid_d\n",
    "    elif activation == 'softmax':\n",
    "        backward_activation_func = softmax_d\n",
    "    else:\n",
    "        raise Expection('Activation function not supported')\n",
    "        \n",
    "    dZ_curr = backward_activation_func(dA, Z)\n",
    "    dW_curr = np.dot(dZ_curr, A.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Yh, Y, mem, params_vals, nn_architecture):\n",
    "    grads_vals = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Yh.shape)\n",
    "\n",
    "    dA_prev = -(np.divide(Y, Yh) - np.divide(1 - Y, 1 - Yh))\n",
    "\n",
    "    for layer_id_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_id_curr = layer_id_prev + 1\n",
    "        activ_function_curr = layer['activation']\n",
    "        aA_curr = dA_prev\n",
    "\n",
    "        A_prev = memory[\"A\" + str(layer_id_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_id_curr)]\n",
    "        W_curr = params_vals[\"W\" + str(layer_id_curr)]\n",
    "        b_curr = params_vals[\"b\" + str(layer_id_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_vals[\"dW\" + str(layer_id_curr)] = dW_curr\n",
    "        grads_vals[\"db\" + str(layer_id_curr)] = db_curr\n",
    "    \n",
    "    return grads_values\n",
    "            \n",
    "def update(params_vals, grads_vals, nn_architecture, learning_rate):\n",
    "    for layer_id, layer in enumerate(nn_architecture):\n",
    "        params_vals[\"W\" + str(layer_id)] -= learning_rate * grads_vals[\"dW\" + str(layer_id)]        \n",
    "        params_vals[\"b\" + str(layer_id)] -= learning_rate * grads_vals[\"db\" + str(layer_id)]\n",
    "\n",
    "    return params_vals;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_vals = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Yh, cashe = full_forward_propagation(X, params_vals, nn_architecture)\n",
    "        cost = get_cost_value(Yh, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Yh, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        grads_vals = full_backward_propagation(Yh, Y, cashe, params_vals, nn_architecture)\n",
    "        params_vals = update(params_vals, grads_vals, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_vals, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,2) and (1797,64) not aligned: 2 (dim 1) != 1797 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-1a849c01310b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-a96cbacc59e6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mYh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcashe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cost_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mcost_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-56cebc32db57>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[1;34m(X, params_vals, nn_architecture)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mW_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mb_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_vals\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mA_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactiv_function_curr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mmem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-f4df0ea5cc40>\u001b[0m in \u001b[0;36msingle_layer_forward_propagation\u001b[1;34m(A_prev, W_curr, b_curr, activation)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mZ_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,2) and (1797,64) not aligned: 2 (dim 1) != 1797 (dim 0)"
     ]
    }
   ],
   "source": [
    "train(X, y, nn_architecture, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "        dataset_split = list()\n",
    "        dataset_copy = list(dataset)\n",
    "        fold_size = int(len(dataset) / n_folds)\n",
    "        for i in range(n_folds):\n",
    "            fold = list()\n",
    "            while len(fold) < fold_size:\n",
    "                index = randrange(len(dataset_copy))\n",
    "                fold.append(dataset_copy.pop(index))\n",
    "            dataset_split.append(fold)\n",
    "        return dataset_split\n",
    "    \n",
    " # Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual)) * 100.0\n",
    "    \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "        folds = cross_validation_split(dataset, n_folds)\n",
    "        scores = list()\n",
    "        for fold in folds:\n",
    "            train_set = list(folds)\n",
    "            train_set.remove(fold)\n",
    "            train_set = sum(train_set, [])\n",
    "            test_set = list()\n",
    "            for row in fold:\n",
    "                row_copy = list(row)\n",
    "                test_set.append(row_copy)\n",
    "                row_copy[-1] = None\n",
    "            predicted = algorithm(train_set, test_set, *args)\n",
    "            actual = [row[-1] for row in fold]\n",
    "            accuracy = accuracy_metric(actual, predicted)\n",
    "            scores.append(accuracy)\n",
    "        return scores\n",
    "    \n",
    " # Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "        activation = weights[-1]\n",
    "        for i in range(len(weights)-1):\n",
    "            activation += weights[i] * inputs[i]\n",
    "        return activation\n",
    "    \n",
    " # Transfer neuron activation\n",
    "def transfer(activation):\n",
    "        return 1.0 / (1.0 + exp(-activation))\n",
    "    \n",
    " # Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "        inputs = row\n",
    "        for layer in network:\n",
    "            new_inputs = []\n",
    "            for neuron in layer:\n",
    "                activation = activate(neuron['weights'], inputs)\n",
    "                neuron['output'] = transfer(activation)\n",
    "                new_inputs.append(neuron['output'])\n",
    "            inputs = new_inputs\n",
    "        return inputs\n",
    "    \n",
    " # Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "        return output * (1.0 - output)\n",
    "    \n",
    " # Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "        for i in reversed(range(len(network))):\n",
    "            layer = network[i]\n",
    "            errors = list()\n",
    "            if i != len(network)-1:\n",
    "                for j in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in network[i + 1]:\n",
    "                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "                \n",
    " # Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "        for i in range(len(network)):\n",
    "            inputs = row[:-1]\n",
    "            if i != 0:\n",
    "                inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "            for neuron in network[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "                neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "                \n",
    " # Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "        for epoch in range(n_epoch):\n",
    "            for row in train:\n",
    "                outputs = forward_propagate(network, row)\n",
    "                expected = [0 for i in range(n_outputs)]\n",
    "                expected[row[-1]] = 1\n",
    "                backward_propagate_error(network, expected)\n",
    "                update_weights(network, row, l_rate)\n",
    "                \n",
    " # Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "        network = list()\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "        network.append(hidden_layer)\n",
    "        output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "        network.append(output_layer)\n",
    "        return network\n",
    "    \n",
    " # Make a prediction with a network\n",
    "def predict(network, row):\n",
    "        outputs = forward_propagate(network, row)\n",
    "        return outputs.index(max(outputs))\n",
    "    \n",
    " # Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "        n_inputs = len(train[0]) - 1\n",
    "        n_outputs = len(set([row[-1] for row in train]))\n",
    "        network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "        train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "        predictions = list()\n",
    "        for row in test:\n",
    "            prediction = predict(network, row)\n",
    "            predictions.append(prediction)\n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Backprop on Seeds dataset\n",
    "# evaluate algorithmn_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 2\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
