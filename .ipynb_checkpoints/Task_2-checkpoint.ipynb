{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for AI - Coursework Task 2\n",
    "\n",
    "The second task is about classifying handwritten digits. We will use the MNIST dataset for training and testing. The point of this task is to develop a multi-layer neural network for classificationusing mostly Numpy:\n",
    "\n",
    "• Implement sigmoid and relu layers (with forward and backward pass)\n",
    "\n",
    "• Implement a softmax output layer\n",
    "\n",
    "• Implement a fully parameterizable neural network (number and types of layers, number of units)\n",
    "\n",
    "• Implement an optimizer(e.g. SGD or Adam)and a stopping criterionof your choosing\n",
    "• Train your Neural Network using backpropagation. Evaluate different neural network architectures andcompare your different results. You can also compare withthe results presented inhttp://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import limited libraries\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "df = load_digits()\n",
    "\n",
    "# Divorce data and target \n",
    "X = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset - Inspiration taken from Lab 04 \n",
    "ratio = 0.75\n",
    "length = len(X)\n",
    "\n",
    "X_train = X[:int(length*ratio)]\n",
    "X_test = X[int(length*ratio):]\n",
    "y_train = y[:(length - int(length*ratio))]\n",
    "y_test = y[(length - int(length*ratio)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Dataset\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "# Sigmoid function\n",
    "    \n",
    "def Sigmoid_forwards(inputs):\n",
    "    return 1/(1+np.exp(-inputs))\n",
    "\n",
    "    # Derivative of Sigmoid (for backward propagation) \n",
    "def Sigmoid_backwards(derivative, inputs):\n",
    "    sig = sigmoid_forwards(inputs)\n",
    "    return derivative * sig * (1 - sig)\n",
    "\n",
    "# ReLu function\n",
    "\n",
    "def Relu_forward(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "# Derivative of ReLu\n",
    "def Relu_backward(derivative, inputs):\n",
    "    derivative = np.array(derivative, copy = True)\n",
    "    derivative[inputs <= 0] = 0;\n",
    "    return derivative;\n",
    "\n",
    "# Softmax function for output\n",
    "def Softmax_forward(self):\n",
    "    e = exp(self.inputs)\n",
    "    return e / e.sum()\n",
    "\n",
    "# Derivative of softmax\n",
    "def Softmax_backward(derivative, inputs):\n",
    "    soft = softmax(inputs)\n",
    "    return derivative * soft * (1 - soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def get_cost(target, output):\n",
    "    m = target.shape[1]\n",
    "    cost = -1 / m * (np.dot(output, np.log(target).T) + np.dot(1 - output, np.log(1 - target).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, nn_architecture):\n",
    "        \n",
    "        self.nn_architecture = nn_architecture\n",
    "        \n",
    "    def initialize(self):\n",
    "    \n",
    "        memory = {}\n",
    "        \n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_number = idx + 1\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "            memory['Weight' + str(layer_number)] = 0.1 * np.random.randn(\n",
    "            layer_output_size, layer_input_size)\n",
    "            memory['bias' + str(layer_number)] = 0.1 * np.random.randn(\n",
    "            layer_output_size, 1)\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, inputs, weights, biases, activation):\n",
    "    \n",
    "        output_forward = np.dot(inputs, weights) + biases\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            activation_function = Sigmoid_forward()\n",
    "        elif(activation == 'relu'):\n",
    "            activation_function = Relu_forward()\n",
    "        elif(activation == 'softmax'):\n",
    "            activation_function = Softmax_forward()\n",
    "        else:\n",
    "            raise Exception('Activation function not defined')\n",
    "            \n",
    "        return activation_function(output_forward), output_forward\n",
    "    \n",
    "    def forward_full(self, X, memory):\n",
    "        temp = {}\n",
    "        inputs_current = X\n",
    "        \n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            layer_number = idx + 1\n",
    "            inputs_previous = inputs_current\n",
    "            \n",
    "            activation_function_current = layer['activation']\n",
    "            weight_current = memory['Weight' + str(layer_number)]\n",
    "            bias_current = memory['bias' + str(layer_number)]\n",
    "            output_forward, result_forward = self.forward(inputs_previous, weight_current, bias_current,\n",
    "                                                         activation_function_current)\n",
    "            \n",
    "            temp['Output' + str(layer_number)] = output_forward\n",
    "            temp['Result' + str(layer_number)] = result_forward\n",
    "            \n",
    "            \n",
    "        return inputs_current, temp\n",
    "    \n",
    "    def backward(self, inputs):\n",
    "        m = inputs.shape\n",
    "    \n",
    "        if(activation == 'sigmoid'):\n",
    "            activation_function = Sigmoid_backward()\n",
    "        elif(activation == 'relu'):\n",
    "            activation_function = Relu_backward()\n",
    "        elif(activation == 'softmax'):\n",
    "            activation_function = Softmax_backward()\n",
    "        else:\n",
    "            raise Exception('Activation function not defined')\n",
    "            \n",
    "        output_backwards = activation_function()\n",
    "        weight_backwards = np.dot().T / m\n",
    "        bias_backwards = np.sum() / m\n",
    "        resul_backwards\n",
    "    \n",
    "    def full_backward(self, target, output, memory, temp, nn_architecture)\n",
    "    \n",
    "        derived_values = {}\n",
    "        \n",
    "        m = output.shape[1]\n",
    "        output = output.reshape(target.shape)\n",
    "        output_error_previous =  np.divide(1 - output, 1 - target) - (np.divide(output, target)\n",
    "        \n",
    "        for layer_number_previous, layer in reversed(list(enumerate(nn_architecture))):\n",
    "            layer_number_current = layer_number_previous + 1\n",
    "            activation_function = layer[\"activation\"]\n",
    "            \n",
    "            output_error_current = output_error_previous\n",
    "            \n",
    "        outpt_previous = memory[\"Output\" + str(layer_number_previous)]\n",
    "        result_current = memory[\"Result\" + str(layer_number_current)]\n",
    "        weight_current = params_values[\"Weight\" + str(layer_number_current)]\n",
    "        bias_current = params_values[\"bias\" + str(layer_number_current)]\n",
    "        \n",
    "        \n",
    "        error =  np.divide(1 - output, 1 - target) - (np.divide(output, target)\n",
    "    def update_weight(self):\n",
    "        pass;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weight1': array([[ 0.17640523,  0.04001572],\n",
       "        [ 0.0978738 ,  0.22408932],\n",
       "        [ 0.1867558 , -0.09772779],\n",
       "        [ 0.09500884, -0.01513572]]),\n",
       " 'bias1': array([[-0.01032189],\n",
       "        [ 0.04105985],\n",
       "        [ 0.01440436],\n",
       "        [ 0.14542735]]),\n",
       " 'Weight2': array([[ 0.07610377,  0.0121675 ,  0.04438632,  0.03336743],\n",
       "        [ 0.14940791, -0.02051583,  0.03130677, -0.08540957],\n",
       "        [-0.25529898,  0.06536186,  0.08644362, -0.0742165 ],\n",
       "        [ 0.22697546, -0.14543657,  0.00457585, -0.01871839],\n",
       "        [ 0.15327792,  0.14693588,  0.01549474,  0.03781625],\n",
       "        [-0.08877857, -0.19807965, -0.03479121,  0.0156349 ]]),\n",
       " 'bias2': array([[ 0.12302907],\n",
       "        [ 0.12023798],\n",
       "        [-0.03873268],\n",
       "        [-0.03023028],\n",
       "        [-0.1048553 ],\n",
       "        [-0.14200179]]),\n",
       " 'Weight3': array([[-0.17062702,  0.19507754, -0.05096522, -0.04380743, -0.12527954,\n",
       "          0.07774904],\n",
       "        [-0.16138978, -0.02127403, -0.08954666,  0.03869025, -0.05108051,\n",
       "         -0.11806322],\n",
       "        [-0.00281822,  0.04283319,  0.00665172,  0.03024719, -0.06343221,\n",
       "         -0.03627412],\n",
       "        [-0.06724604, -0.03595532, -0.08131463, -0.17262826,  0.01774261,\n",
       "         -0.04017809],\n",
       "        [-0.16301983,  0.04627823, -0.09072984,  0.00519454,  0.07290906,\n",
       "          0.01289829],\n",
       "        [ 0.11394007, -0.12348258,  0.04023416, -0.06848101, -0.08707971,\n",
       "         -0.05788497]]),\n",
       " 'bias3': array([[-0.03115525],\n",
       "        [ 0.00561653],\n",
       "        [-0.11651498],\n",
       "        [ 0.09008265],\n",
       "        [ 0.04656624],\n",
       "        [-0.15362437]]),\n",
       " 'Weight4': array([[ 0.14882522,  0.18958892,  0.11787796, -0.01799248, -0.10707526,\n",
       "          0.10544517],\n",
       "        [-0.04031769,  0.12224451,  0.0208275 ,  0.0976639 ,  0.03563664,\n",
       "          0.07065732],\n",
       "        [ 0.00105   ,  0.17858705,  0.01269121,  0.04019894,  0.18831507,\n",
       "         -0.13477591],\n",
       "        [-0.1270485 ,  0.09693967, -0.11731234,  0.19436212, -0.0413619 ,\n",
       "         -0.07474548]]),\n",
       " 'bias4': array([[0.1922942 ],\n",
       "        [0.14805148],\n",
       "        [0.1867559 ],\n",
       "        [0.09060447]]),\n",
       " 'Weight5': array([[-0.08612257,  0.1910065 , -0.02680034,  0.08024564]]),\n",
       " 'bias5': array([[0.0947252]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(nn_architecture)\n",
    "nn.initialize()\n",
    "nn.full_forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.01423588,  0.20572217],\n",
      "       [ 0.02832619,  0.1329812 ],\n",
      "       [-0.01546219, -0.00690309],\n",
      "       [ 0.07551805,  0.08256466]]), 'b1': array([[-0.01130692],\n",
      "       [-0.23678376],\n",
      "       [-0.01670494],\n",
      "       [ 0.0685398 ]]), 'W2': array([[ 0.00235001,  0.04562013,  0.02704928, -0.14350081],\n",
      "       [ 0.08828171, -0.05800817, -0.05015653,  0.05909533],\n",
      "       [-0.07316163,  0.02617555, -0.08557956, -0.01875259],\n",
      "       [-0.03734863, -0.0461971 , -0.08164661, -0.00451233],\n",
      "       [ 0.01213278,  0.09259528, -0.05738197,  0.00527031],\n",
      "       [ 0.22073106,  0.03918219,  0.04827134,  0.0433334 ]]), 'b2': array([[-0.17042917],\n",
      "       [-0.02439081],\n",
      "       [-0.21397038],\n",
      "       [ 0.08613227],\n",
      "       [ 0.17002844],\n",
      "       [-0.05287848]]), 'W3': array([[ 0.17634779, -0.11216078, -0.11919342,  0.05527319, -0.08159809,\n",
      "        -0.04966468],\n",
      "       [ 0.10862256, -0.09746753, -0.02821358, -0.01172141,  0.03785473,\n",
      "         0.07321946],\n",
      "       [-0.0103571 , -0.11987063,  0.10100356,  0.28753603,  0.08203126,\n",
      "         0.05606115],\n",
      "       [-0.03756422, -0.02521043, -0.13896134,  0.06173323, -0.0135787 ,\n",
      "         0.1287905 ],\n",
      "       [-0.10369944,  0.13643321, -0.03099566, -0.06111171, -0.04831058,\n",
      "        -0.06089837],\n",
      "       [-0.20883353,  0.0639322 ,  0.0774304 ,  0.12785694,  0.0705276 ,\n",
      "         0.06559774]]), 'b3': array([[-0.1678502 ],\n",
      "       [ 0.01831099],\n",
      "       [-0.11332241],\n",
      "       [-0.02790857],\n",
      "       [ 0.13966199],\n",
      "       [ 0.00322194]]), 'W4': array([[-0.26136608, -0.10015776, -0.0567511 , -0.0225658 ,  0.09380238,\n",
      "         0.08367841],\n",
      "       [ 0.08121485,  0.0232307 , -0.02951077, -0.0361676 ,  0.04321151,\n",
      "         0.09339585],\n",
      "       [ 0.15526339,  0.00936234,  0.02948258,  0.14854308, -0.10868852,\n",
      "         0.08211628],\n",
      "       [-0.07879492,  0.15938117,  0.14059044,  0.16447566,  0.15415987,\n",
      "         0.08406076]]), 'b4': array([[-0.10230944],\n",
      "       [ 0.04947723],\n",
      "       [ 0.08957326],\n",
      "       [ 0.0477352 ]]), 'W5': array([[-0.01145305,  0.01568974,  0.03875967, -0.10262266]]), 'b5': array([[0.06791429]])}\n"
     ]
    }
   ],
   "source": [
    "layers = init_layers(nn_architecture)\n",
    "print(layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
